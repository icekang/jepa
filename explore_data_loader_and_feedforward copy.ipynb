{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.video_classification_frozen.eval import make_dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "from src.models.utils.patch_embed import PatchEmbed3D\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from app.vjepa.utils import (\n",
    "    init_video_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robust_pca(features: torch.Tensor, m: float = 2, remove_first_component=False):\n",
    "    # features: (N, C)\n",
    "    # m: a hyperparam controlling how many std dev outside for outliers\n",
    "    assert len(features.shape) == 2, \"features should be (N, C)\"\n",
    "    reduction_mat = torch.pca_lowrank(features, q=3, niter=20)[2]\n",
    "    colors = features @ reduction_mat\n",
    "    if remove_first_component:\n",
    "        colors_min = colors.min(dim=0).values\n",
    "        colors_max = colors.max(dim=0).values\n",
    "        tmp_colors = (colors - colors_min) / (colors_max - colors_min)\n",
    "        fg_mask = tmp_colors[..., 0] < 0.2\n",
    "        reduction_mat = torch.pca_lowrank(features[fg_mask], q=3, niter=20)[2]\n",
    "        colors = features @ reduction_mat\n",
    "    else:\n",
    "        fg_mask = torch.ones_like(colors[:, 0]).bool()\n",
    "    d = torch.abs(colors[fg_mask] - torch.median(colors[fg_mask], dim=0).values)\n",
    "    mdev = torch.median(d, dim=0).values\n",
    "    s = d / mdev\n",
    "    try:\n",
    "        rins = colors[fg_mask][s[:, 0] < m, 0]\n",
    "        gins = colors[fg_mask][s[:, 1] < m, 1]\n",
    "        bins = colors[fg_mask][s[:, 2] < m, 2]\n",
    "        rgb_min = torch.tensor([rins.min(), gins.min(), bins.min()])\n",
    "        rgb_max = torch.tensor([rins.max(), gins.max(), bins.max()])\n",
    "    except:\n",
    "        rins = colors\n",
    "        gins = colors\n",
    "        bins = colors\n",
    "        rgb_min = torch.tensor([rins.min(), gins.min(), bins.min()])\n",
    "        rgb_max = torch.tensor([rins.max(), gins.max(), bins.max()])\n",
    "\n",
    "    return reduction_mat, rgb_min.to(reduction_mat), rgb_max.to(reduction_mat)\n",
    "\n",
    "\n",
    "def get_pca_map(\n",
    "    feature_map: torch.Tensor,\n",
    "    img_size,\n",
    "    interpolation=\"bicubic\",\n",
    "    return_pca_stats=False,\n",
    "    pca_stats=None,\n",
    "    remove_first_component=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    feature_map: (1, h, w, C) is the feature map of a single image.\n",
    "    \"\"\"\n",
    "    # print(feature_map.shape)\n",
    "    if feature_map.shape[0] != 1:\n",
    "        # make it (1, h, w, C)\n",
    "        feature_map = feature_map[None]\n",
    "    if pca_stats is None:\n",
    "        reduct_mat, color_min, color_max = get_robust_pca(\n",
    "            feature_map.reshape(-1, feature_map.shape[-1]),\n",
    "            remove_first_component=remove_first_component,\n",
    "        )\n",
    "    else:\n",
    "        reduct_mat, color_min, color_max = pca_stats\n",
    "    pca_color = feature_map @ reduct_mat\n",
    "    pca_color = (pca_color - color_min) / (color_max - color_min)\n",
    "    pca_color = pca_color.clamp(0, 1)\n",
    "    pca_color = F.interpolate(\n",
    "        pca_color.permute(0, 3, 1, 2),\n",
    "        size=img_size,\n",
    "        mode=interpolation,\n",
    "    ).permute(0, 2, 3, 1)\n",
    "    pca_color = pca_color.cpu().numpy().squeeze(0)\n",
    "    if return_pca_stats:\n",
    "        return pca_color, (reduct_mat, color_min, color_max)\n",
    "    return pca_color\n",
    "\n",
    "def get_pca_map_whole_volume(\n",
    "    feature_map: torch.Tensor,\n",
    "    img_size,\n",
    "    interpolation=\"bicubic\",\n",
    "    return_pca_stats=False,\n",
    "    pca_stats=None,\n",
    "    remove_first_component=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    feature_map: (num_frames, h, w, C) is the feature map of a single image.\n",
    "    \"\"\"\n",
    "    # print(feature_map.shape)\n",
    "    if feature_map.shape[0] != 1:\n",
    "        # make it (1, num_frames, h, w, C)\n",
    "        feature_map = feature_map[None]\n",
    "    if pca_stats is None:\n",
    "        reduct_mat, color_min, color_max = get_robust_pca(\n",
    "            feature_map.reshape(-1, feature_map.shape[-1]),\n",
    "            remove_first_component=remove_first_component,\n",
    "        )\n",
    "    else:\n",
    "        reduct_mat, color_min, color_max = pca_stats\n",
    "    pca_color = feature_map @ reduct_mat\n",
    "    pca_color = (pca_color - color_min) / (color_max - color_min)\n",
    "    pca_color = pca_color.clamp(0, 1)\n",
    "    resized_pca_colors = []\n",
    "    for i in range(pca_color.shape[1]):\n",
    "        resized_pca_color = F.interpolate(\n",
    "            pca_color[:, i, :, :, :].permute(0, 3, 1, 2),\n",
    "            size=img_size,\n",
    "            mode=interpolation,\n",
    "        ).permute(0, 2, 3, 1)\n",
    "        resized_pca_colors.append(resized_pca_color.cpu().numpy().squeeze(0))\n",
    "    pca_color = np.stack(resized_pca_colors, axis=0)\n",
    "    if return_pca_stats:\n",
    "        return pca_color, (reduct_mat, color_min, color_max)\n",
    "    return pca_color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Patch3D Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = torch.randn(1, 3, 16, 224, 224)\n",
    "x = PatchEmbed3D(patch_size=16, tubelet_size=16, in_chans=3)(clip)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "224 * 224 * 16 / 16 / 16 / 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_path = ['/storage_bizon/naravich/Unlabeled_OCT_videos/Unlabel_OCT_Video.csv']\n",
    "batch_size = 1\n",
    "eval_frames_per_clip = 16\n",
    "eval_frame_step = 1\n",
    "duration = None\n",
    "num_clips=1\n",
    "dataset_type = 'VideoDataset'\n",
    "resolution = 512\n",
    "eval_duration = None\n",
    "eval_num_segments = 8\n",
    "attend_across_segments = True\n",
    "\n",
    "world_size = 1\n",
    "rank = 0\n",
    "\n",
    "data_loader = make_dataloader(\n",
    "        dataset_type=dataset_type,\n",
    "        root_path=train_data_path,\n",
    "        resolution=resolution,\n",
    "        frames_per_clip=eval_frames_per_clip,\n",
    "        frame_step=eval_frame_step,\n",
    "        eval_duration=eval_duration,\n",
    "        num_segments=eval_num_segments if attend_across_segments else 1,\n",
    "        num_views_per_segment=1,\n",
    "        allow_segment_overlap=True,\n",
    "        batch_size=batch_size,\n",
    "        world_size=world_size,\n",
    "        rank=rank,\n",
    "        training=False)\n",
    "\n",
    "for data in data_loader:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data[0]), type(data[1]), type(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][0][0].shape), data[1].shape, data[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = data[0][0][0].min()\n",
    "max_val = data[0][0][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = ((data[0][0][0] - min_val) / (max_val - min_val))[0].permute(1, 2, 3, 0).numpy()\n",
    "plt.imshow((frame[4] * 255).astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame[4].sum(axis=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs/pretrain/vitl16.yaml', 'r') as y_file:\n",
    "    args = yaml.load(y_file, Loader=yaml.FullLoader)\n",
    "\n",
    "# -- set device\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "# -- META\n",
    "cfgs_meta = args.get('meta')\n",
    "use_sdpa = cfgs_meta.get('use_sdpa', False)\n",
    "\n",
    "# -- MODEL\n",
    "cfgs_model = args.get('model')\n",
    "model_name = cfgs_model.get('model_name')\n",
    "pred_depth = cfgs_model.get('pred_depth')\n",
    "pred_embed_dim = cfgs_model.get('pred_embed_dim')\n",
    "uniform_power = cfgs_model.get('uniform_power', True)\n",
    "use_mask_tokens = cfgs_model.get('use_mask_tokens', True)\n",
    "zero_init_mask_tokens = cfgs_model.get('zero_init_mask_tokens', True)\n",
    "\n",
    "# -- MASK\n",
    "cfgs_mask = args.get('mask')\n",
    "\n",
    "# -- DATA\n",
    "cfgs_data = args.get('data')\n",
    "dataset_type = cfgs_data.get('dataset_type', 'videodataset')\n",
    "mask_type = cfgs_data.get('mask_type', 'multiblock3d')\n",
    "dataset_paths = cfgs_data.get('datasets', [])\n",
    "datasets_weights = cfgs_data.get('datasets_weights', None)\n",
    "if datasets_weights is not None:\n",
    "    assert len(datasets_weights) == len(dataset_paths), 'Must have one sampling weight specified for each dataset'\n",
    "batch_size = cfgs_data.get('batch_size')\n",
    "batch_size = 1\n",
    "num_clips = cfgs_data.get('num_clips')\n",
    "num_frames = cfgs_data.get('num_frames')\n",
    "tubelet_size = cfgs_data.get('tubelet_size')\n",
    "sampling_rate = cfgs_data.get('sampling_rate')\n",
    "duration = cfgs_data.get('clip_duration', None)\n",
    "crop_size = cfgs_data.get('crop_size', 224)\n",
    "patch_size = cfgs_data.get('patch_size')\n",
    "pin_mem = cfgs_data.get('pin_mem', False)\n",
    "num_workers = cfgs_data.get('num_workers', 1)\n",
    "filter_short_videos = cfgs_data.get('filter_short_videos', False)\n",
    "decode_one_clip = cfgs_data.get('decode_one_clip', True)\n",
    "log_resource_util_data = cfgs_data.get('log_resource_utilization', False)\n",
    "attend_across_segments = False\n",
    "world_size = 1\n",
    "rank = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = ['lol.csv']\n",
    "# train_data_path = ['/storage_bizon/naravich/Unlabeled_OCT_videos/Unlabel_OCT_Video.csv']\n",
    "data_loader = make_dataloader(\n",
    "        dataset_type=dataset_type,\n",
    "        root_path=train_data_path,\n",
    "        resolution=crop_size,\n",
    "        frames_per_clip=num_frames,\n",
    "        frame_step=sampling_rate,\n",
    "        eval_duration=duration,\n",
    "        num_segments=eval_num_segments if attend_across_segments else 1,\n",
    "        num_views_per_segment=1,\n",
    "        allow_segment_overlap=True,\n",
    "        batch_size=batch_size,\n",
    "        world_size=world_size,\n",
    "        rank=rank,\n",
    "        training=False)\n",
    "\n",
    "for data in data_loader:\n",
    "    clips, masks_enc, masks_pred = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips[0][0].shape\n",
    "min_val = clips[0][0][0].permute(1, 2, 3, 0)[0].numpy().min()\n",
    "max_val = clips[0][0][0].permute(1, 2, 3, 0)[0].numpy().max()\n",
    "img = (clips[0][0][0].permute(1, 2, 3, 0)[0].numpy() - min_val) / (max_val - min_val)\n",
    "print(img.min(), img.max())\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, predictor = init_video_model(\n",
    "    uniform_power=uniform_power,\n",
    "    use_mask_tokens=use_mask_tokens,\n",
    "    num_mask_tokens=len(cfgs_mask),\n",
    "    zero_init_mask_tokens=zero_init_mask_tokens,\n",
    "    device=device,\n",
    "    patch_size=patch_size,\n",
    "    num_frames=num_frames,\n",
    "    tubelet_size=tubelet_size,\n",
    "    model_name=model_name,\n",
    "    crop_size=crop_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_embed_dim=pred_embed_dim,\n",
    "    use_sdpa=use_sdpa,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load('vitl16.pth.tar', map_location='cpu')\n",
    "# checkpoint = torch.load('vith16.pth.tar', map_location='cpu')\n",
    "print(checkpoint.keys())\n",
    "new_encoder_state_dict = {}\n",
    "pretrained_dict = checkpoint['target_encoder']\n",
    "pretrained_dict = {k.replace('module.', ''): v for k, v in pretrained_dict.items()}\n",
    "# pretrained_dict = {k.replace('backbone.', ''): v for k, v in pretrained_dict.items()}\n",
    "encoder.load_state_dict(pretrained_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.backbone.pos_embed = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoder(clips[0][0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, N, D = x.shape\n",
    "B, C, FRAMES, H, W = clips[0][0].shape\n",
    "print(B, C, FRAMES, H, W)\n",
    "print(x.shape)\n",
    "print(H * W * FRAMES / 16 / 16 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_of_vjepa = x\n",
    "\n",
    "\n",
    "print('output_of_vjepa:', output_of_vjepa.shape)\n",
    "print('input shape:', clips[0][0].shape)\n",
    "\n",
    "B, C, FRAMES, H, W = clips[0][0].shape\n",
    "# Patch = (tubelet_size, patch_size, patch_size)\n",
    "N_FRAMES = FRAMES // tubelet_size\n",
    "N_H = H // patch_size\n",
    "N_W = W // patch_size\n",
    "\n",
    "print(f'Thus, N feature ({output_of_vjepa.shape[1]}) is calcuated from', H * W * FRAMES / tubelet_size / patch_size / patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMES // tubelet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FRAMES = FRAMES // tubelet_size\n",
    "N_H = H // patch_size\n",
    "N_W = W // patch_size\n",
    "\n",
    "# feats = x.reshape(B, N_FRAMES, N_H, N_W, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feat_by_depth(feats, depth):\n",
    "    B, N, D = feats.shape\n",
    "    B, C, FRAMES, H, W = clips[0][0].shape\n",
    "    N_FRAMES = FRAMES // tubelet_size\n",
    "    N_H = H // patch_size\n",
    "    N_W = W // patch_size\n",
    "    feats = feats.reshape(B, N_FRAMES, N_H, N_W, D)\n",
    "    return feats[:, depth, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_index = 0\n",
    "def show_side_by_side_feature_map_by_clip_index(clip_index):\n",
    "    feat = select_feat_by_depth(x.detach(), clip_index)\n",
    "    image_size = (crop_size, crop_size)\n",
    "    image = clips[0][0][0].permute(1, 2, 3, 0)[clip_index].numpy()\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    pca_map = get_pca_map(feat, image_size, interpolation=\"bilinear\")\n",
    "    pca_map_2 = get_pca_map(feat, image_size, interpolation=\"bilinear\", remove_first_component=True)\n",
    "\n",
    "    sbs = np.concatenate([image, pca_map, pca_map_2], axis=1)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(sbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_side_by_side_feature_map_by_clip_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_pca_map_whole_volume()\n",
    "image_size = (crop_size, crop_size)\n",
    "volumne_pca_map =  get_pca_map_whole_volume(x.detach().reshape(batch_size, N_FRAMES, N_H, N_W, D), image_size, interpolation=\"bilinear\", remove_first_component=False)\n",
    "print(volumne_pca_map.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes, fig = plt.subplots(2, 8, figsize=(40, 20))\n",
    "for i in range(8):\n",
    "    fig[0, i].imshow(volumne_pca_map[i])\n",
    "\n",
    "for clip_index in range(8):\n",
    "    image = clips[0][0][0].permute(1, 2, 3, 0)[clip_index].numpy()\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    fig[1, clip_index].imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jepa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
